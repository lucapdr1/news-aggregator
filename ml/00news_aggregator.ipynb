{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Aggregator\n",
    "\n",
    "## Features\n",
    "- **Top Articles Retrieval**: Scrapes top articles from predefined news sources.\n",
    "- **Article Summarization**: Summarizes articles to provide concise representations.\n",
    "- **Sentiment Detection**: Determines the sentiment of articles based on sentiment analysis.\n",
    "- **Topic Modeling**: Describes with three words the topic of the articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newspaper\n",
    "import json\n",
    "\n",
    "newspaper_list = [ 'https://time.com/',\n",
    "        'https://www.theguardian.com/europe',\n",
    "        'https://edition.cnn.com/' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Articles Retrieval\n",
    "The script scrapes the top articles from predefined news sources listed in the `newspaper_list`. It utilizes the `newspaper` library to build newspaper objects from these URLs. The `get_top_articles` function takes a newspaper URL and retrieves a specified number of top articles (default is 5) from it. It then extracts relevant information from each article, including its title, publication date, text content, and URL. This feature enables the aggregation of recent articles from multiple news sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\P2001\\anaconda3\\envs\\dw\\lib\\site-packages\\newspaper\\source.py:260: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if feed.doc:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'newspaper': 'https://time.com/', 'title': 'Parent Company of Saks Fifth Avenue to Buy Rival Neiman Marcus', 'date': '2024-07-04', 'text': \"The parent company of Saks Fifth Avenue has signed a deal to buy upscale rival Neiman Marcus for $2.65 billion.\\n\\nThe new entity would be called Saks Global, which will comprise the Saks Fifth Avenue and Saks OFF 5TH brands, Neiman Marcus and Bergdorf Goodman, as well as the real estate assets of Neiman Marcus Group and HBC, a holding company that purchased Saks in 2013.\\n\\nThe pact was announced Thursday after months of rumors that the department store chains had been negotiating a deal.\\n\\nThe Wall Street Journal first reported the impending deal Wednesday.\\n\\nBoth Saks and Neiman Marcus have struggled as shoppers have been pulling back on buying high-end goods and shifting their spending toward experiences, like travel and upscale restaurants. The two iconic luxury purveyors have also faced stiffer competition from luxury brands, which are increasingly opening their own stores. The deal would help reduce operating costs and create more negotiating power with vendors.\\n\\nSaks Fifth Avenue currently operates 39 stores in the U.S., including its Manhattan flagship. In early 2021, Saks spun off its website into a separate company, with the hopes of expanding that business at a time when more people were shopping online.\\n\\nNeiman Marcus filed for bankruptcy protection in May 2020 during the first months of the coronavirus pandemic but emerged in September of that year. Like many of its peers, the privately held department store chain was forced to temporarily close its stores for several months.\\n\\nMeanwhile, other department stores are under pressure to keep increasing sales.\\n\\nStoried Lord & Taylor announced in late August 2020 it was closing all its stores after filing for bankruptcy earlier that month. It's operating online. Macy’s announced in February of this year that it will close 150 unproductive namesake stores over the next three years including 50 by year-end.\\n\\nConsumers have proven resilient and willing to shop even after a bout of inflation, though behaviors have shifted, with some Americans trading down to lower-priced goods.\\n\\nA deal between the two luxury retailers does not resolve all the issues, especially when high-end shoppers are looking to buy luxury goods online or at luxury brands' own stores, Saunders said.\\n\\n“As a larger entity, negotiating power will be a little better with the brands, but even a combined chain would not match the heft and power of the global luxury conglomerates, which would still hold most of the cards,” Saunders said. “As such, there is a risk that the deal might end up creating an even bigger headache for Saks.”\", 'url': 'https://time.com/6995155/parent-company-of-saks-fifth-avenue-to-buy-rival-neiman-marcus-as-department-stores-struggle/'}\n"
     ]
    }
   ],
   "source": [
    "def get_top_articles(newspaper_url, num_articles=5):\n",
    "    paper = newspaper.build(newspaper_url,number_threads=3)\n",
    "    top_articles = []\n",
    "\n",
    "    article_urls = [article.url for article in paper.articles[:num_articles]]\n",
    "\n",
    "    for article in paper.articles[:num_articles]:\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        top_articles.append(article)\n",
    "        article.url\n",
    "    \n",
    "    return zip(article_urls, top_articles)\n",
    "\n",
    "articles_info = []\n",
    "\n",
    "for news_url in newspaper_list:\n",
    "    top_articles = get_top_articles(news_url, num_articles=5)\n",
    "  \n",
    "    # Display article titles\n",
    "    for url, article in top_articles:\n",
    "        single_info = {\n",
    "            \"newspaper\": str(news_url),\n",
    "            \"title\": article.title,\n",
    "            \"date\": article.publish_date.strftime('%Y-%m-%d') if article.publish_date else None,\n",
    "            \"text\" : article.text,\n",
    "            \"url\": str(url)\n",
    "        }\n",
    "        articles_info.append(single_info)\n",
    "\n",
    "# Convert the list to a JSON object\n",
    "articles_json = json.dumps(articles_info)\n",
    "\n",
    "# Print the JSON object of the first one to show it is correctly structured\n",
    "print(articles_info[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Summarization\n",
    "For article summarization, the script employs the `summarize` function. This function takes an article URL as input and utilizes the `newspaper` library to download, parse, and perform natural language processing (NLP) on the article. By leveraging NLP, it generates a summary of the article's content. This summary provides a condensed representation of the article's main points, making it easier for users to grasp the essential information without having to read the entire article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The parent company of Saks Fifth Avenue has signed a deal to buy upscale rival Neiman Marcus for $2.65 billion.\\nThe new entity would be called Saks Global, which will comprise the Saks Fifth Avenue and Saks OFF 5TH brands, Neiman Marcus and Bergdorf Goodman, as well as the real estate assets of Neiman Marcus Group and HBC, a holding company that purchased Saks in 2013.\\nBoth Saks and Neiman Marcus have struggled as shoppers have been pulling back on buying high-end goods and shifting their spending toward experiences, like travel and upscale restaurants.\\nThe two iconic luxury purveyors have also faced stiffer competition from luxury brands, which are increasingly opening their own stores.\\nNeiman Marcus filed for bankruptcy protection in May 2020 during the first months of the coronavirus pandemic but emerged in September of that year.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def summarize(article_url):\n",
    "    article = newspaper.article(article_url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    return article.summary\n",
    "\n",
    "#Example usage\n",
    "summarize(articles_info[0]['url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Detection\n",
    "The script includes a `detect_sentiment` function that determines the political bias of articles based on sentiment analysis. It utilizes the `transformers` library to load a sentiment analysis model pretrained on text from different languages. The sentiment analysis model assigns a sentiment score to the text, indicating the overall sentiment (positive, negative, or neutral). Based on this score, the function categorizes the article's political bias as 'left-leaning,' 'right-leaning,' or 'neutral.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\P2001\\anaconda3\\envs\\dw\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def detect_sentiment(text):\n",
    "    # Load sentiment analysis model with explicit model and revision\n",
    "    sentiment_model = pipeline(model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "    # Perform sentiment analysis on the text\n",
    "    sentiment = sentiment_model(text)\n",
    "\n",
    "    # Based on sentiment score, determine sentiment\n",
    "    sentiment_label = sentiment[0]['label']\n",
    "    sentiment_score = float(sentiment_label.split()[0])\n",
    "\n",
    "    if sentiment_score > 3:\n",
    "        return 'positive'\n",
    "    elif sentiment_score < 3:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "detect_sentiment(articles_info[0]['text'][:511])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "The script includes a `get_topic` function that determines the main topics of articles using topic modeling. It utilizes the `sklearn` library to load a Latent Dirichlet Allocation (LDA) model. The `preprocess_text` function cleans the text by removing non-alphabet characters, converting to lowercase, and eliminating stopwords. The text is then transformed into a document-term matrix using `CountVectorizer`. The LDA model identifies the dominant topics within the text, and the `extract_topics` function retrieves the top words representing these topics. This approach helps in summarizing the main themes of the text, making it easier to understand the central topics of the article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\P2001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'saks stores luxury'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Preprocessing function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove non-alphabet characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "# Display Topics\n",
    "def extract_topics(model, feature_names, no_top_words):\n",
    "    topicString = \"\"\n",
    "    for topic in model.components_:\n",
    "        topicString = \" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "    return topicString\n",
    "\n",
    "\n",
    "\n",
    "def get_topic(text_to_process):\n",
    "    # Preprocess the document\n",
    "    preprocessed_text = preprocess_text(text_to_process)\n",
    "\n",
    "    # Vectorization\n",
    "    vectorizer = CountVectorizer(max_df=2, min_df=0.85, stop_words='english')\n",
    "    dtm = vectorizer.fit_transform([preprocessed_text])\n",
    "\n",
    "    # LDA Model\n",
    "    lda = LatentDirichletAllocation(n_components=1, random_state=42)\n",
    "    lda.fit(dtm)\n",
    "\n",
    "    number_top_words = 3\n",
    "    return extract_topics(lda, vectorizer.get_feature_names_out(), number_top_words)\n",
    "\n",
    "get_topic(articles_info[0]['text'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
